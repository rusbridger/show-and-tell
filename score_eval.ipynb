{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a4ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d62eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoCaptions\n",
    "from pycocoevalcap.eval import COCOEvalCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0747a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.20s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(\"./data/annotations/captions_val2014.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a675cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = \"./results/captions_model_val.json\"\n",
    "out_file = \"./results/val2014_scores.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baa11df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycocoevalcap.eval.COCOEvalCap at 0x165e077aa90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocoEval.setEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c442837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
      "Progress: 384.5M / 384.5M (100.0%)\n",
      "Extracting stanford-corenlp-3.6.0 ...\n",
      "Done.\n",
      "computing Bleu score...\n",
      "{'testlen': 397719, 'reflen': 392242, 'guess': [397719, 357215, 316711, 276207], 'correct': [257502, 117369, 47951, 20289]}\n",
      "ratio: 1.0139633185635373\n",
      "Bleu_1: 0.647\n",
      "Bleu_2: 0.461\n",
      "Bleu_3: 0.318\n",
      "Bleu_4: 0.221\n",
      "computing METEOR score...\n",
      "METEOR: 0.214\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.475\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.682\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpql7trnex', '-cache', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpwwzi9qgd', '-subset', '-silent']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9b4ad435862e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcocoEval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCOCOEvalCap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoco_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcocoEval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoco_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetImgIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcocoEval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m indices = [\"BLEU 1-gram\", \"BLEU 2-gram\", \"BLEU 3-gram\", \"BLEU 4-gram\",\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pycocoevalcap\\eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'computing %s score...'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pycocoevalcap\\spice\\spice.py\u001b[0m in \u001b[0;36mcompute_score\u001b[1;34m(self, gts, res)\u001b[0m\n\u001b[0;32m     73\u001b[0m           \u001b[1;34m'-silent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         ]\n\u001b[1;32m---> 75\u001b[1;33m         subprocess.check_call(spice_cmd, \n\u001b[0m\u001b[0;32m     76\u001b[0m             cwd=os.path.dirname(os.path.abspath(__file__)))\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpql7trnex', '-cache', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'C:\\\\Users\\\\Shayan\\\\anaconda3\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpwwzi9qgd', '-subset', '-silent']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# evaluate best captions against gt\n",
    "coco_result = coco.loadRes(res_file)\n",
    "cocoEval = COCOEvalCap(coco, coco_result)\n",
    "cocoEval.params['image_id'] = coco_result.getImgIds()\n",
    "cocoEval.evaluate()\n",
    "\n",
    "indices = [\"BLEU 1-gram\", \"BLEU 2-gram\", \"BLEU 3-gram\", \"BLEU 4-gram\",\n",
    "           \"METEOR\", \"ROUGE_L\", \"CIDEr\", \"SPICE\"]\n",
    "data = [cocoEval.eval['Bleu_1']] + [cocoEval.eval['Bleu_2']] + [cocoEval.eval['Bleu_3']] + [cocoEval.eval['Bleu_4']] + \\\n",
    "       [cocoEval.eval['METEOR']] + [cocoEval.eval['ROUGE_L']] + [cocoEval.eval['CIDEr']]\n",
    "results = pd.DataFrame(columns=[f\"k={k}_Train_num={train_early_stop}_Val_num={val_early_stop}\"], index=indices, data=data)\n",
    "results.to_excel(out_file)\n",
    "print(f\"Results saved to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52467e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
